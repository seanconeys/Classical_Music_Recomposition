{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all packages and song to be used for generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Documents\\\\Computer Science\\\\Computer Music\\\\Final\\\\Chopin.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bf08009664bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Here we load the song that will be used as the basis for our recomposition and display it for listening\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Chopin.wav'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mipd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\audioread\\__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrawread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mrawread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRawAudioFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\audioread\\rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \"\"\"\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\user\\\\Documents\\\\Computer Science\\\\Computer Music\\\\Final\\\\Chopin.wav'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "import random\n",
    "import pretty_midi\n",
    "import numpy\n",
    "import scipy \n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa, librosa.display\n",
    "from math import sqrt\n",
    "\n",
    "# Here we load the song that will be used as the basis for our recomposition and display it for listening\n",
    "original = './Chopin7.wav'\n",
    "x, sr = librosa.load(original) \n",
    "ipd.Audio(data=original, rate=sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in pitch detection is to detect onsets. This tells us when a new sound begins and then gives us starting indicies for each note. In order to calculate an estimate of the length of note N we create onset boundaries. We then estimate the pitch for each period between onsets and output a list pitches containing the list of the pitches of each period and leghts showing the lengths of each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onset_samples = librosa.onset.onset_detect(x,\n",
    "                                           sr=sr, units='samples', \n",
    "                                           hop_length=100, \n",
    "                                           backtrack=False,\n",
    "                                           pre_max=20,\n",
    "                                           post_max=20,\n",
    "                                           pre_avg=100,\n",
    "                                           post_avg=100,\n",
    "                                           delta=0.2,\n",
    "                                           wait=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onset_boundaries = numpy.concatenate([[0], onset_samples, [len(x)]])\n",
    "#print(onset_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onset_times = librosa.samples_to_time(onset_boundaries, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_pitch(segment, sr, lowest_freq=50.0, highest_freq=2000.0):\n",
    "    correlation = librosa.autocorrelate(segment)\n",
    "    minimum = int(sr/ highest_freq)\n",
    "    maximum = int(sr/lowest_freq)\n",
    "    correlation[:minimum] = 0\n",
    "    correlation[maximum:] = 0\n",
    "    i = correlation.argmax()\n",
    "    freq = float(sr)/i\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_pitch2(x, onset_samples, i, sr):\n",
    "    start = onset_samples[i]\n",
    "    end = onset_samples[i+1]\n",
    "    freq = estimate_pitch(x[start:end], sr)\n",
    "    return (freq)\n",
    "def pitch_length(x, onset_samples, i, sr):\n",
    "    start = onset_samples[i]\n",
    "    end = onset_samples[i+1]\n",
    "    return (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pitches = []\n",
    "lengths = []\n",
    "for i in range(len(onset_boundaries)-1):\n",
    "    pitches.append(estimate_pitch2(x, onset_boundaries, i, sr))\n",
    "    lengths.append(pitch_length(x, onset_boundaries, i, sr))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert the frequency of each note to a string of the note name with octave removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notes = [librosa.hz_to_note(i) for i in pitches]\n",
    "formatted_notes = [note[0] if note[1] != '#' else note[0:2] for note in notes ]\n",
    "length_dict = {}\n",
    "for i in range(len(formatted_notes)):\n",
    "    try:\n",
    "        length_dict[formatted_notes[i]].append(lengths[i])\n",
    "    except:\n",
    "        length_dict[formatted_notes[i]] =[lengths[i]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the total time each note is played in the composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile = {'A':0, 'A#':0, 'B':0, 'C':0, 'C#':0, 'D':0, 'D#':0,'E':0, 'F':0, 'F#':0 ,'G':0, 'G#':0}\n",
    "for i in length_dict.keys():\n",
    "    time = 0\n",
    "    for j in length_dict[i]:\n",
    "        time += j\n",
    "    \n",
    "    profile[i] = time/sr\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "major_profile = [6.35,2.23,3.48,2.33,4.38,4.09,2.52,5.19,2.39,3.66,2.29,2.88]\n",
    "minor_profile = [6.33,2.68,3.52,5.38,2.60,3.53,2.54,4.75,3.98,2.69,3.34,3.17]\n",
    "note_list = ['A','A#','B','C','C#','D','D#','E','F','F#','G','G#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_coefficient(x,y):\n",
    "    x_mean = numpy.mean(x)\n",
    "    y_mean = numpy.mean(y)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    x_summation = 0\n",
    "    y_summation = 0\n",
    "    for i in range(len(x)):\n",
    "        numerator += (x[i]-x_mean)*(y[i]-y_mean)\n",
    "    for i in range(len(x)):\n",
    "        x_summation += (x[i]-x_mean)**2\n",
    "        y_summation += (y[i]-y_mean)**2\n",
    "    return(numerator/(sqrt(x_summation*y_summation)))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in key detection is to loop over each key profile and calulate its correlation(Pearson Corr with our note durations. The key profile that has the highest correlation with our note distribution will be our best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = {}\n",
    "for index in range(len(note_list)):\n",
    "    new_profile = []\n",
    "    for i in note_list[index:]:\n",
    "        new_profile.append(profile[i])\n",
    "    for i in note_list[:index]:\n",
    "        new_profile.append(profile[i])\n",
    "    #print(new_profile)\n",
    "    #print(calc_coefficient(new_profile,major_profile))\n",
    "    matches[calc_coefficient(new_profile,major_profile)] = ''.join([str(note_list[index]),'_major'])\n",
    "    matches[calc_coefficient(new_profile,minor_profile)] = ''.join([str(note_list[index]),'_minor'])\n",
    "    \n",
    "best_match = max(matches.keys())\n",
    "key =matches[best_match]\n",
    "print(key)\n",
    "print(best_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then find the BPM (Beats per Minute) which is the second factor we will use in our nearest neighbor search. We do this using a librosa utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bpm = librosa.beat.tempo(x)\n",
    "print(round(int(bpm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use a naive K-Nearest Neighbors search to find most closely related songs in our library. This is complicated by the fact that distance for keys is circular, A minor is first in our list and G# major is last in our list but they have a distance of 1. To account for this we compute both the distance from point1 to point2 in the traditional sense and then compute the distance \"around the back\" and return the minimum of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_to_value = ['A_minor','A_major','A#_minor','A#_major','B_minor','B_major','C_minor','C_major','C#_minor','C#_major','D_minor','D_major','D#_minor','D#_major','E_minor','E_major','F_minor','F_major','F#_minor','F#_major','G_minor','G_major','G#_minor','G#_major']\n",
    "def euclidean_dist(point1, point2):\n",
    "    x_dist = int(point1[1]) - int(point2[1])\n",
    "    if key_to_value.index(point1[0]) < key_to_value.index(point2[0]):\n",
    "        y_dist = min(key_to_value.index(point2[0]) - key_to_value.index(point1[0]), (len(key_to_value)-key_to_value.index(point2[0]))+key_to_value.index(point1[0]))\n",
    "    elif key_to_value.index(point1[0]) > key_to_value.index(point2[0]):\n",
    "        y_dist = min(key_to_value.index(point1[0]) - key_to_value.index(point2[0]), (len(key_to_value)-key_to_value.index(point1[0]))+key_to_value.index(point2[0]))\n",
    "    else:\n",
    "        y_dist = 0\n",
    "    return(sqrt((x_dist**2)+(y_dist**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songlist = [['./musicdr/chopin1.wav', 'G#_major', 135], ['./musicdr/chopin2.wav', 'G#_major', 89], ['./musicdr/haydn1.wav', 'G#_major', 129], ['./musicdr/haydn2.wav', 'G#_major', 129], ['./musicdr/haydn3.wav', 'G#_major', 117], ['./musicdr/mozart1.wav', 'A_major', 103], ['./musicdr/mozart2.wav', 'A_major', 143], ['./musicdr/mozart3.wav', 'A_major', 123], ['./musicdr/schu1.wav', 'A_minor', 129], ['./musicdr/schu2.wav', 'A_minor', 103], ['./musicdr/schu3.wav', 'A_minor', 123], ['./musicdr/bee1.wav', 'A#_major', 107], ['./musicdr/bee2.wav', 'A#_major', 112], ['./musicdr/bee3.wav', 'A#_major', 92], ['./musicdr/bee4.wav', 'A#_major', 135], ['./musicdr/bee5.wav', 'A#_major', 161], ['./musicdr/bee6.wav', 'A#_major', 129], ['./musicdr/bee8.wav', 'A#_major', 172], ['./musicdr/mozart4.wav', 'A#_major', 129], ['./musicdr/mozart5.wav', 'A#_major', 112], ['./musicdr/mozart6.wav', 'A#_major', 161], ['./musicdr/mozart7.wav', 'A#_major', 161], ['./musicdr/mozart8.wav', 'A#_major', 92], ['./musicdr/mozart9.wav', 'A#_major', 143], ['./musicdr/schu4.wav', 'A#_major', 95], ['./musicdr/schu5.wav', 'A#_major', 129], ['./musicdr/schu6.wav', 'A#_major', 135], ['./musicdr/chopin3.wav', 'A#_minor', 95], ['./musicdr/chopin4.wav', 'A#_minor', 112], ['./musicdr/chopin5.wav', 'A#_minor', 117], ['./musicdr/chopin6.wav', 'A#_minor', 123], ['./musicdr/chopin7.wav', 'A#_minor', 112], ['./musicdr/bach1.wav', 'C_major', 151], ['./musicdr/brahms1.wav', 'C_major', 112], ['./musicdr/brahms2.wav', 'C_major', 78], ['./musicdr/brahms3.wav', 'C_major', 172], ['./musicdr/brahms4.wav', 'C_major', 129], ['./musicdr/haydn4.wav', 'C_major', 89], ['./musicdr/haydn5.wav', 'C_major', 112], ['./musicdr/haydn6.wav', 'C_major', 112], ['./musicdr/haydn7.wav', 'C_major', 161], ['./musicdr/haydn8.wav', 'C_major', 103], ['./musicdr/haydn9.wav', 'C_major', 99], ['./musicdr/mozart10.wav', 'C_major', 135], ['./musicdr/mozart11.wav', 'C_major', 92], ['./musicdr/mozart12.wav', 'C_major', 172], ['./musicdr/mozart13.wav', 'C_major', 135], ['./musicdr/mozart14.wav', 'C_major', 112], ['./musicdr/mozart15.wav', 'C_major', 95], ['./musicdr/schu7.wav', 'C_major', 143], ['./musicdr/schu8.wav', 'C_major', 123], ['./musicdr/schu9.wav', 'C_major', 89], ['./musicdr/schu10.wav', 'C_major', 112], ['./musicdr/wald1.wav', 'C_major', 103], ['./musicdr/wald2.wav', 'C_major', 89], ['./musicdr/wald3.wav', 'C_major', 107], ['./musicdr/bach2.wav', 'C_minor', 129], ['./musicdr/bee9.wav', 'C_minor', 184], ['./musicdr/bee10.wav', 'C_minor', 69], ['./musicdr/bee11.wav', 'C_minor', 99], ['./musicdr/pa1.wav', 'C_minor', 151], ['./musicdr/pa2.wav', 'C_minor', 107], ['./musicdr/pa3.wav', 'C_minor', 198], ['./musicdr/mond1.wav', 'C#_minor', 143], ['./musicdr/mond2.wav', 'C#_minor', 103], ['./musicdr/mond3.wav', 'C#_minor', 161], ['./musicdr/rac1.wav', 'C#_minor', 61], ['./musicdr/bach3.wav', 'D_major', 143], ['./musicdr/haydn10.wav', 'D_major', 95], ['./musicdr/haydn11.wav', 'D_major', 129], ['./musicdr/haydn12.wav', 'D_major', 123], ['./musicdr/mozart16.wav', 'D_major', 143], ['./musicdr/mozart17.wav', 'D_major', 89], ['./musicdr/mozart18.wav', 'D_major', 95], ['./musicdr/schu11.wav', 'D_major', 172], ['./musicdr/schu12.wav', 'D_major', 129], ['./musicdr/schu13.wav', 'D_major', 151], ['./musicdr/schu14.wav', 'D_major', 112], ['./musicdr/bee12.wav', 'D#_major', 123], ['./musicdr/bee13.wav', 'D#_major', 123], ['./musicdr/bee14.wav', 'D#_major', 107], ['./musicdr/bee15.wav', 'E_minor', 151], ['./musicdr/bee16.wav', 'E_minor', 143], ['./musicdr/ba1.wav', 'F_major', 83], ['./musicdr/haydn13.wav', 'F_major', 143], ['./musicdr/haydn14.wav', 'F_major', 123], ['./musicdr/haydn15.wav', 'F_major', 117], ['./musicdr/mozart19.wav', 'F_major', 143], ['./musicdr/mozart20.wav', 'F_major', 143], ['./musicdr/mozart21.wav', 'F_major', 99], ['./musicdr/ap1.wav', 'F_minor', 117], ['./musicdr/ap2.wav', 'F_minor', 99], ['./musicdr/ap3.wav', 'F_minor', 143], ['./musicdr/haydn16.wav', 'G_major', 184], ['./musicdr/haydn17.wav', 'G_major', 161], ['./musicdr/haydn18.wav', 'G_major', 172], ['./musicdr/haydn19.wav', 'G_major', 99], ['./musicdr/haydn20.wav', 'G_major', 103], ['./musicdr/haydn21.wav', 'G_major', 107], ['./musicdr/ba2.wav', 'G_minor', 129], ['./musicdr/ba3.wav', 'G_major', 123], ['./musicdr/chopin8.wav', 'G_major', 123]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find the most closely related songs in database using naive k-nearest neighbors search that calculates the euclidean distance from our original piece to all pieces in our database and returns the k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "distances = {}\n",
    "for song in songlist:\n",
    "    distances[euclidean_dist([key,bpm],(song[1],song[2]))] = song[0]\n",
    "neighbors = []\n",
    "for neighbor in range(k):\n",
    "    neighbors.append(distances.pop(min(distances.keys())))\n",
    "neighbors.append(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the songs we are going to pull from we can begin our recomposition. For our recomposition we loop over each song creating windows of size window_size and estimating their tempo. We then combine these windows into samples or or all possible groupings of contigous windows of size sample_size. We then compute the average tempo and the std of each sample. We then divide these into 3 groups slow, medium, or fast based off their tempo with samples having low std's first. The idea is that the lower the std the lower changes in speed is across the sample indicating that all windows inside of it are likely stylistically simillar. We can then choose these stylistically simillar samples for recomposition based off of their tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activity = []\n",
    "window_size = 2\n",
    "\n",
    "for song in neighbors:\n",
    "    x, sr = librosa.load(song) \n",
    "    onset_samples = librosa.onset.onset_detect(x,\n",
    "                                               sr=sr, units='samples', \n",
    "                                               hop_length=100, \n",
    "                                               backtrack=False,\n",
    "                                               pre_max=20,\n",
    "                                               post_max=20,\n",
    "                                               pre_avg=100,\n",
    "                                               post_avg=100,\n",
    "                                               delta=0.2,\n",
    "                                               wait=0)\n",
    "    onset_boundaries = numpy.concatenate([[0], onset_samples, [len(x)]])\n",
    "   \n",
    "    \n",
    "    index = 0\n",
    "    for i in range(0,int(len(x)/sr),window_size):\n",
    "        window_start=(i*sr)\n",
    "        window_end=(i+window_size)*sr\n",
    "        count=0\n",
    "        while(onset_boundaries[index] < window_end):\n",
    "            count+=1\n",
    "            index+=1\n",
    "            if(index == len(onset_boundaries)):\n",
    "                count = 0\n",
    "                break\n",
    "\n",
    "        activity.append([count,window_start,song])\n",
    "#print(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 3\n",
    "sample_metrics = []\n",
    "\n",
    "for i in range(len(activity)-sample_size):\n",
    "    sample_metrics.append([numpy.std([window[0] for  window in activity[i:i+3]]),numpy.mean([window[0] for  window in activity[i:i+3]]),activity[i][1],activity[i][2]])\n",
    "\n",
    "    \n",
    "maximum = max([sample[1] for sample in sample_metrics])\n",
    "minimum = min([sample[1] for sample in sample_metrics])\n",
    "difference = maximum-minimum\n",
    "slow = [sample for sample in sample_metrics if (0<sample[1] < (minimum + (difference/3)))]\n",
    "medium = [sample  for sample in sample_metrics if (minimum + (difference/3)) <= sample[1] < (minimum + (2*(difference/3)))]\n",
    "fast = [sample for sample in sample_metrics if (minimum + (2*(difference/3)) <= sample[1] )]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
